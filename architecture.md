SmartDocs AI â€” System Architecture

SmartDocs AI is a lightweight, retrieval-augmented document assistant that processes PDF documents, extracts insights, and provides context-aware responses using Groqâ€™s Llama-3.1 model.
This document explains the architecture, data flow, and design decisions behind the system.

ğŸ§© High-Level Architecture Overview
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚        Streamlit UI       â”‚
              â”‚  (Upload, Chat, Summary)  â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                 User Input / PDF Upload
                             â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚    document_loader.py     â”‚
              â”‚  - PDF Parsing (PyPDF2)   â”‚
              â”‚  - Text Extraction         â”‚
              â”‚  - Chunking Engine         â”‚
              â”‚  - Metadata Computation    â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                        Extracted Text
                             â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚       chat_engine.py      â”‚
              â”‚  - Build contextual prompt â”‚
              â”‚  - Summary generation      â”‚
              â”‚  - Keyword extraction      â”‚
              â”‚  - Groq Llama-3.1 API call â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                     Model Response
                             â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚       Streamlit UI        â”‚
              â”‚  - Chat bubbles UI        â”‚
              â”‚  - Summary tab            â”‚
              â”‚  - Insights tab           â”‚
              â”‚  - Search mode            â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Core Modules
1ï¸âƒ£ app.py â€” Main Application Layer

Handles all user-facing interactions:

PDF upload UI

Sidebar navigation

Routing between Chat / Summary / Search / Insights

Rendering all output from Groq API

Responsibilities:
Function	Purpose
load_pdf()	Reads PDF and stores extracted text in session
show_chat_ui()	Renders chat-style interface
show_summary_ui()	Displays auto-generated summary
show_insights()	Shows stats like keywords, pages, word count
show_search_ui()	Searches inside chunks
2ï¸âƒ£ document_loader.py â€” Document Processing Layer
Key Functions:
Function	Description
extract_text_from_pdf()	Converts PDF â†’ raw text using PyPDF2
chunk_text()	Splits long text into semantic chunks
get_metadata()	Computes page count, word count, etc.

This layer is responsible for making the text structured and model-friendly.

3ï¸âƒ£ chat_engine.py â€” AI Processing Layer

Powered by Groq Llama-3.1-8B-Instant.

Responsibilities:

Builds contextual prompts

Performs Q&A by injecting relevant chunks

Generates summaries

Extracts keywords

Ensures the model answers ONLY from the document

Key Methods:
Method	Purpose
answer_question()	Context-aware retrieval + Groq completion
generate_summary()	Summaries from full doc
extract_keywords()	Keyword extraction
âš¡ Key Design Decisions
1ï¸âƒ£ Why Groq Llama-3.1?

Free tier

No quota limits

Fast inference

Modern 2025 LLM architecture

Perfect for student challenge submissions

2ï¸âƒ£ Why Chunking?

LLMs cannot take entire huge PDFs at once.

Chunking:

Preserves context

Prevents model hallucination

Enables precise Q&A

Makes search faster

3ï¸âƒ£ Retrieval-Augmented Prompting

Each user question:

Runs keyword match

Selects only relevant chunks

Injects them into prompt

Gives factual, document-based answers

ğŸ“Š Data Flow Diagram
PDF â†’ Text Extractor â†’ Chunker â†’ Stored in Session  
User Query â†’ Chunk Selector â†’ Prompt Builder â†’ Groq Model  
Groq Response â†’ UI Renderer (Chat, Summary, Insights)

ğŸ§ª Model Used
Model: llama-3.1-8b-instant
Provider: Groq Cloud
Format: Chat Completions API
Features Used:

Causal inference

Instruction following

Token-efficient summaries

Fast context reasoning

ğŸ” Security & Key Management

.env file stores API keys locally

.env is ignored via .gitignore

No keys are committed to GitHub

Safe for public submission

ğŸ Conclusion

SmartDocs AI is a clean, modular, and fast prototype demonstrating:

Document parsing

Retrieval-augmented Q&A

Multi-mode AI interface

Real-time insights

Production-style structure

This architecture is scalable and can easily expand into:

Multi-document memory

Vector database (Faiss / Chroma)

Multi-modal file support

Async processing